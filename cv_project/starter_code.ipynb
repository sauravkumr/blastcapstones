{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Autonomous CV Project\n",
        "\n",
        "In this project, you will work on a computer vision task for autonomous driving. This project will guide you through the steps of preparing the data, building a deep learning model, training the model, and making predictions.\n",
        "\n",
        "## Project Steps Overview:\n",
        "1. **Data Preprocessing:** Load and preprocess the image data.\n",
        "2. **Data Augmentation:** Apply augmentation techniques to increase the dataset variability.\n",
        "3. **Model Building:** Define the CNN architecture using a deep learning framework.\n",
        "4. **Model Training:** Train the CNN model with the prepared data.\n",
        "5. **Prediction:** Use the trained model to make predictions on test data.\n",
        "\n",
        "Let's get started! Provided below is some code to help you get set up with the data and imports."
      ],
      "metadata": {
        "id": "TAuyFfO2v_QK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries\n",
        "First, import the necessary libraries for data manipulation, visualization, and model building."
      ],
      "metadata": {
        "id": "HSdA6Q44wPgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YfDJDsG0GKi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLl8pEM1if1E",
        "outputId": "ec00cf55-2735-47f9-d748-bbaf4f65aace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Dataset\n",
        "\n",
        "Download and load the image dataset, and apply the necessary transformations. Make sure to adjust the path to your dataset."
      ],
      "metadata": {
        "id": "u6n5gtulwcES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data transforms\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "nlxMrvK7GewT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_dataset = ImageFolder(root=\"/content/drive/MyDrive/GTSRB/Train\", transform=data_transforms)"
      ],
      "metadata": {
        "id": "sVzyfMeBGgMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a smaller portion of the dataset\n",
        "portion = 0.05  # Use 10% of the dataset for training and validation\n",
        "total_size = len(train_dataset)\n",
        "subset_size = int(portion * total_size)\n",
        "indices = np.random.choice(total_size, subset_size, replace=False)"
      ],
      "metadata": {
        "id": "KkWxDw-foAQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the subset indices into training and validation sets\n",
        "train_size = int(0.8 * subset_size)\n",
        "val_size = subset_size - train_size\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:]"
      ],
      "metadata": {
        "id": "oAp6XGWOsPmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Data for CNN\n",
        "Create subsets for your train and validation data using the `Subset` from PyTorch, and then create the data loaders using PyTorch's `DataLoader`"
      ],
      "metadata": {
        "id": "XM-m4zPzw_qL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BB_SmoK0oOMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fHpU71fbGjfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building\n",
        "Define the CNN model architecture:\n",
        "1. **Define the model**: Create a class or function to define the CNN layers and architecture (hint: try using 3 convolutional layers, and play around with kernel_size and padding)\n",
        "3. **Compile the model**: Initialize the model, specify the loss function, optimizer, and evaluation metrics."
      ],
      "metadata": {
        "id": "vXG7ebzOx5Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Fq-EO56Gl7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hIBAX-sWGoRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b3SgbMRqGp6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "Train the CNN model with the prepared data:\n",
        "1. **Implement the training loop**: Iterate over the training data, perform forward and backward passes, and update the model weights.\n",
        "2. **Monitor the training**: Track the training loss and accuracy to ensure the model is learning."
      ],
      "metadata": {
        "id": "QsomNsI_yczc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTIiptAGGrYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction\n",
        "Use the trained model to make predictions on test data:\n",
        "1. **Implement the prediction function**: Use the trained model to generate predictions on the validation data (hint: you may need to loop over the images and labels provided in the `val_loader`)\n",
        "2. **Evaluate the model's performance**: Calculate metrics like accuracy, precision, recall, etc., to assess the model's performance.\n",
        "3. **Visualize the results**: Display the predicted vs actual labels to visually inspect the model's performance.\n",
        "4. **Try predicting on an actual image**: Provide an image to the model to make a prediction"
      ],
      "metadata": {
        "id": "f_b5-v1dynRo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4-jSXMCGvmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HBaZqJUG0Bc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}